{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import pstats\n",
    "import multiprocessing\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 40000  # Only consider the top 30k words\n",
    "maxlen = 130  # Max sequence size\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=1.0)\n",
    "\n",
    "    model.compile(\n",
    "        'adam', loss=[loss_fn, None], metrics=['accuracy']\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data_chunks = []\n",
    "    for i in range(0, len(data), 512):\n",
    "        data_chunks.append(data[i:i+512])\n",
    "\n",
    "    return data_chunks\n",
    "\n",
    "def encode_data(data):\n",
    "    encoded_chunks = []\n",
    "    for chunk in data:\n",
    "        encoded = tokenizer.encode(chunk, max_length=512, truncation=True, add_special_tokens=False)\n",
    "        encoded_chunks += encoded\n",
    "\n",
    "    return encoded_chunks\n",
    "\n",
    "def encode_chunk(chunk):\n",
    "\n",
    "    encoded = tokenizer.encode(chunk, max_length=512, truncation=True, add_special_tokens=False)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def encode_data(data):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        encoded_chunks = pool.map(encode_chunk, data)\n",
    "    \n",
    "    return [encoded for chunk in encoded_chunks for encoded in chunk]\n",
    "\n",
    "def dataset():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", max_vocab_size=vocab_size)\n",
    "\n",
    "    path = os.getcwd()\n",
    "    dir = path.replace('Transformer', 'WikipediaScraper')\n",
    "    os.chdir(dir)\n",
    "\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    columns = df['article'].tolist()\n",
    "    joined_data = '\\n'.join(str(columns))\n",
    "    \n",
    "    splited = split_data(joined_data)\n",
    "    \n",
    "    encoded = encode_data(splited)\n",
    "    encoded_chunks = encoded\n",
    "\n",
    "    print(len(encoded_chunks))\n",
    "\n",
    "    sequenced_data = []\n",
    "    for i in range(0, len(encoded_chunks) - maxlen, 3):\n",
    "        sequenced_data.append(encoded_chunks[i : i + maxlen + 1])\n",
    "\n",
    "    sequenced_data = tf.random.shuffle(sequenced_data)\n",
    "    print(sequenced_data)\n",
    "    \n",
    "    #get token vocabs\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    subword_list = [None] * len(vocab)\n",
    "\n",
    "    for subword, index in vocab.items():\n",
    "        subword_list[index] = subword\n",
    "    vocab = subword_list\n",
    "\n",
    "    def prepare_lm_inputs_labels(text):\n",
    "\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        text = tf.convert_to_tensor(text, dtype=tf.int32)\n",
    "        x = text[:, :-1]\n",
    "        y = text[:, 1:]\n",
    "        return x, y\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(sequenced_data)\n",
    "    text_ds = text_ds.shuffle(buffer_size=200000)\n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "    text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return df, text_ds, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, text_ds, vocab = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data_chunks = []\n",
    "    for i in range(0, len(data) - maxlen, 512):\n",
    "        data_chunks.append(data[i:i+512])\n",
    "\n",
    "    return data_chunks\n",
    "\n",
    "def encode_data(data):\n",
    "    encoded_chunks = []\n",
    "    \n",
    "    for chunk in data:\n",
    "        encoded = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        encoded_chunks += encoded\n",
    "\n",
    "    return encoded_chunks\n",
    "\n",
    "def encode_chunk(chunk):\n",
    "\n",
    "    encoded = tokenizer.encode(chunk, max_length=512, truncation=True, add_special_tokens=False)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def encode_data(data):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        encoded_chunks = pool.map(encode_chunk, data)\n",
    "    \n",
    "    return [encoded for chunk in encoded_chunks for encoded in chunk]\n",
    "\n",
    "def save_data(data):\n",
    "    conn = sqlite3.connect('data.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE random_numbers\n",
    "                 (id INTEGER PRIMARY KEY,\n",
    "                 number INTEGER)''')\n",
    "    for num in data:\n",
    "        c.execute(f\"INSERT INTO random_numbers (number) VALUES ({num})\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def dataset():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", max_vocab_size=vocab_size)\n",
    "\n",
    "    path = os.getcwd()\n",
    "    dir = path.replace('Transformer', 'WikipediaScraper')\n",
    "    os.chdir(dir)\n",
    "\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    columns = df['article'].tolist()\n",
    "    new_columns = list(map(str, columns))\n",
    "    joined_data = '\\n'.join(new_columns)\n",
    "    \n",
    "    splited = split_data(joined_data)\n",
    "\n",
    "    encoded = encode_data(splited)\n",
    "    encoded_chunks = encoded\n",
    "\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    subword_list = [None] * len(vocab)\n",
    "    for subword, index in vocab.items():\n",
    "        subword_list[index] = subword\n",
    "    vocab = subword_list\n",
    "\n",
    "    save_data(encoded_chunks)\n",
    "\n",
    "    return df, vocab, encoded_chunks, joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, vocab, encoded_chunks, joined_data = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(encoded_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(vocab), len(encoded_chunks), len(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "\n",
    "    def __init__(self,batch_size, maxlen, threadsafe=True, vocab_size=40000):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "        if threadsafe:\n",
    "            self.conn = sqlite3.connect('data.db', check_same_thread=False)\n",
    "        else:\n",
    "            self.conn = sqlite3.connect('data.db')\n",
    "        cursor = self.conn.execute(\"SELECT COUNT(*) FROM random_numbers\")\n",
    "        self.count = cursor.fetchone()[0]\n",
    "        print(self.count)\n",
    "\n",
    "    def random_index(self):\n",
    "        return random.randint(1, self.count - self.maxlen - 1)\n",
    "\n",
    "    def get_data(self):\n",
    "        c = self.conn.cursor()\n",
    "        index = self.random_index()\n",
    "        c.execute(\"SELECT * FROM random_numbers WHERE id BETWEEN ? AND ?\", (index, index + self.maxlen+1))\n",
    "        sequence_data = [row[1] for row in c]\n",
    "        return sequence_data\n",
    "    \n",
    "    def label_data(self, data):\n",
    "\n",
    "        #data = tf.cast(data, dtype=tf.int32)\n",
    "        #data = tf.convert_to_tensor(data, dtype=tf.int32)\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        return x, y\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            sequence = []\n",
    "            x, y = [], []\n",
    "            x_list, y_list = [], []\n",
    "            for _ in range(self.batch_size):\n",
    "                sequence = self.get_data()\n",
    "                x, y = self.label_data(sequence)\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "\n",
    "            if len(x_list) == self.batch_size:\n",
    "                try:\n",
    "                    x_list = np.array(x_list)\n",
    "                    y_list = np.array(y_list)\n",
    "                    yield x_list, y_list\n",
    "                except ValueError:\n",
    "                    print(x_list)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=10\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \"\".join(\n",
    "            tokenizer.decode(self.start_tokens + tokens_generated)\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"Chess is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callbacks = {\n",
    "    'checkpoint': ModelCheckpoint('model-{epoch:03d}.h5', monitor='loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(128, maxlen, threadsafe=True, vocab_size=40000)\n",
    "data_gen = data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = create_model()\n",
    "\n",
    "    steps_size = 1000\n",
    "\n",
    "    if os.path.exists('model-008.h5'):\n",
    "        model.load_weights('model-008.h5')\n",
    "\n",
    "    model.fit(data_gen, steps_per_epoch = steps_size, verbose=1, epochs=2, callbacks=[text_gen_callback, callbacks['checkpoint']])\n",
    "\n",
    "    model.save_weights('transformer.h5')\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    with cProfile.Profile() as pr:\n",
    "        model = train()\n",
    "    stats = pstats.Stats(pr)\n",
    "    stats.sort_stats(pstats.SortKey.TIME)\n",
    "    stats.dump_stats('transformer.prof')\n",
    "    return model\n",
    "\n",
    "model= main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights('model-002.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_text():\n",
    "\n",
    "    x = 'suck my dikc'\n",
    "\n",
    "    def __init__(self, model, max_tokens, start_tokens, index_to_word, top_k=5, print_every=1):\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "    \n",
    "    def sample_from(self, logits):\n",
    "        logits /= 0.1\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "    \n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "    \n",
    "    def generate(self):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[-maxlen:]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            if sample_token == 102:\n",
    "                break\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \"\".join(\n",
    "            tokenizer.decode(self.start_tokens + tokens_generated)\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        print(self.start_tokens + tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "sentence = \"Basketball is\"\n",
    "start_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "print(start_tokens)\n",
    "max_tokens = 200\n",
    "text_gen = generate_text(model, max_tokens, start_tokens, vocab)\n",
    "for i in range(2):\n",
    "    text_gen.generate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
