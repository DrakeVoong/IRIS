{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 14:17:30.742391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 14:17:31.603075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64::/home/drake/Documents/projects/IRIS/.conda/lib/\n",
      "2023-03-25 14:17:31.603134: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64::/home/drake/Documents/projects/IRIS/.conda/lib/\n",
      "2023-03-25 14:17:31.603140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import pstats\n",
    "import multiprocessing\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 14:17:34.156416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-25 14:17:34.178401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-25 14:17:34.178568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000  # Only consider the top 20k words\n",
    "maxlen = 100  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=1.0)\n",
    "\n",
    "    model.compile(\n",
    "        'adam', loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data_chunks = []\n",
    "    for i in range(0, len(data), 512):\n",
    "        data_chunks.append(data[i:i+512])\n",
    "\n",
    "    return data_chunks\n",
    "\n",
    "def encode_data(data):\n",
    "    encoded_chunks = []\n",
    "    for chunk in data:\n",
    "        encoded = tokenizer.encode(chunk, max_length=512, truncation=True, add_special_tokens=False)\n",
    "        encoded_chunks += encoded\n",
    "\n",
    "    return encoded_chunks\n",
    "\n",
    "def encode_chunk(chunk):\n",
    "\n",
    "    encoded = tokenizer.encode(chunk, max_length=512, truncation=True, add_special_tokens=False)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def encode_data(data):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        encoded_chunks = pool.map(encode_chunk, data)\n",
    "    \n",
    "    return [encoded for chunk in encoded_chunks for encoded in chunk]\n",
    "\n",
    "def dataset():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", max_vocab_size=vocab_size)\n",
    "\n",
    "    path = os.getcwd()\n",
    "    dir = path.replace('Transformer', 'WikipediaScraper')\n",
    "    os.chdir(dir)\n",
    "\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    columns = df['article'].tolist()\n",
    "    joined_data = '\\n'.join(str(columns))\n",
    "    \n",
    "    splited = split_data(joined_data)\n",
    "    \n",
    "    encoded = encode_data(splited)\n",
    "    encoded_chunks = encoded\n",
    "\n",
    "    print(len(encoded_chunks))\n",
    "\n",
    "    sequenced_data = []\n",
    "    for i in range(0, len(encoded_chunks) - maxlen, 3):\n",
    "        sequenced_data.append(encoded_chunks[i : i + maxlen + 1])\n",
    "\n",
    "    sequenced_data = tf.random.shuffle(sequenced_data)\n",
    "    print(sequenced_data)\n",
    "    \n",
    "    #get token vocabs\n",
    "    vocab = tokenizer.get_vocab()\n",
    "\n",
    "    subword_list = [None] * len(vocab)\n",
    "\n",
    "    for subword, index in vocab.items():\n",
    "        subword_list[index] = subword\n",
    "    vocab = subword_list\n",
    "\n",
    "    def prepare_lm_inputs_labels(text):\n",
    "\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        text = tf.convert_to_tensor(text, dtype=tf.int32)\n",
    "        x = text[:, :-1]\n",
    "        y = text[:, 1:]\n",
    "        return x, y\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(sequenced_data)\n",
    "    text_ds = text_ds.shuffle(buffer_size=200000)\n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "    text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return df, text_ds, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136800\n",
      "tf.Tensor(\n",
      "[[1012 1044 1041 ... 1057 1038 1059]\n",
      " [1041 1040 1051 ... 1054 1045 1048]\n",
      " [1051 1049 1056 ... 1041 1045 1055]\n",
      " ...\n",
      " [1018 1016 1050 ... 1061 1042 1051]\n",
      " [1052 1041 1050 ... 1042 1045 1054]\n",
      " [1057 1037 1061 ... 1054 1048 1040]], shape=(45567, 101), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "df, text_ds, vocab = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data_chunks = []\n",
    "    for i in range(0, len(data) - maxlen, 512):\n",
    "        data_chunks.append(data[i:i+512])\n",
    "\n",
    "    return data_chunks\n",
    "\n",
    "def encode_data(data):\n",
    "    encoded_chunks = []\n",
    "    \n",
    "    for chunk in data:\n",
    "        encoded = tokenizer.encode(chunk, add_special_tokens=False, max_length=512, truncation=True)\n",
    "        encoded_chunks += encoded\n",
    "\n",
    "    return encoded_chunks\n",
    "\n",
    "def save_data(data):\n",
    "    conn = sqlite3.connect('data.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE random_numbers\n",
    "                 (id INTEGER PRIMARY KEY,\n",
    "                 number INTEGER)''')\n",
    "    for num in data:\n",
    "        c.execute(f\"INSERT INTO random_numbers (number) VALUES ({num})\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def dataset():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", max_vocab_size=vocab_size)\n",
    "\n",
    "    path = os.getcwd()\n",
    "    dir = path.replace('Transformer', 'WikipediaScraper')\n",
    "    os.chdir(dir)\n",
    "\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    columns = df['article'].tolist()\n",
    "    new_columns = list(map(str, columns))\n",
    "    joined_data = '\\n'.join(new_columns)\n",
    "    \n",
    "    splited = split_data(joined_data)\n",
    "\n",
    "    encoded = encode_data(splited)\n",
    "    encoded_chunks = encoded\n",
    "\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    subword_list = [None] * len(vocab)\n",
    "    for subword, index in vocab.items():\n",
    "        subword_list[index] = subword\n",
    "    vocab = subword_list\n",
    "\n",
    "    save_data(encoded_chunks)\n",
    "\n",
    "    return df, vocab, encoded_chunks, joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, vocab, encoded_chunks, joined_data = dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "\n",
    "    def __init__(self,batch_size, maxlen, threadsafe=True, vocab_size=30000):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "        if threadsafe:\n",
    "            self.conn = sqlite3.connect('data.db', check_same_thread=False)\n",
    "        else:\n",
    "            self.conn = sqlite3.connect('data.db')\n",
    "        cursor = self.conn.execute(\"SELECT COUNT(*) FROM random_numbers\")\n",
    "        self.count = cursor.fetchone()[0]\n",
    "        print(self.count)\n",
    "\n",
    "    def random_index(self):\n",
    "        return random.randint(1, self.count - self.maxlen - 1)\n",
    "\n",
    "    def get_data(self):\n",
    "        c = self.conn.cursor()\n",
    "        index = self.random_index()\n",
    "        c.execute(\"SELECT * FROM random_numbers WHERE id BETWEEN ? AND ?\", (index, index + self.maxlen+1))\n",
    "        sequence_data = [row[1] for row in c]\n",
    "        return sequence_data\n",
    "    \n",
    "    def label_data(self, data):\n",
    "\n",
    "        #data = tf.cast(data, dtype=tf.int32)\n",
    "        #data = tf.convert_to_tensor(data, dtype=tf.int32)\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "        return x, y\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            sequence = []\n",
    "            sequences = []\n",
    "            x, y = [], []\n",
    "            x_list, y_list = [], []\n",
    "            for _ in range(self.batch_size):\n",
    "                sequence = self.get_data()\n",
    "                x, y = self.label_data(sequence)\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "\n",
    "            if len(x_list) == self.batch_size:\n",
    "                try:\n",
    "                    x_list = np.array(x_list)\n",
    "                    y_list = np.array(y_list)\n",
    "                    yield x_list, y_list\n",
    "                except ValueError:\n",
    "                    print(x_list)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=10\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"Chess is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439108\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator(64, maxlen)\n",
    "data_gen = data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "400/400 [==============================] - 80s 179ms/step - loss: 3.1535 - dense_47_loss: 3.1535\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 69s 171ms/step - loss: 0.1936 - dense_47_loss: 0.1936\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 67s 169ms/step - loss: 0.1077 - dense_47_loss: 0.1077\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.0901 - dense_47_loss: 0.0901\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 67s 168ms/step - loss: 0.0825 - dense_47_loss: 0.0825\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model = create_model()\n",
    "\n",
    "    steps_size = 400\n",
    "\n",
    "    if os.path.exists('transformer.h5'):\n",
    "        model.load_weights('transformer.h5')\n",
    "\n",
    "    transformer = model.fit(data_gen, steps_per_epoch = steps_size, verbose=1, epochs=5, callbacks=[text_gen_callback])\n",
    "\n",
    "    model.save_weights('transformer.h5')\n",
    "def main():\n",
    "    with cProfile.Profile() as pr:\n",
    "        train()\n",
    "    stats = pstats.Stats(pr)\n",
    "    stats.sort_stats(pstats.SortKey.TIME)\n",
    "    stats.dump_stats('transformer.prof')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights('transformer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/drake/Documents/projects/IRIS/Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_text():\n",
    "\n",
    "    x = 'suck my dikc'\n",
    "\n",
    "    def __init__(self, model, max_tokens, start_tokens, index_to_word, top_k=20, print_every=1):\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "    \n",
    "    def sample_from(self, logits):\n",
    "        logits /= 0.5\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "    \n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "    \n",
    "    def generate(self):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[-maxlen:]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            if sample_token == 102:\n",
    "                break\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \"\".join(\n",
    "            tokenizer.decode(self.start_tokens + tokens_generated)\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "        print(self.start_tokens + tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2900, 2001]\n",
      "generated text:\n",
      "japan was also a stub. you can help wikipedia by expanding it. this article about a library - related building or structure is a stub. you can help wikipedia by expanding it. the 1950 east tenness ee state buccaneers football team was an american football team that represented east tennessee state college ( etsc ) — now known as east tennessee state university — as a member of the smoky mountain conference and the volunteer state athletic conference ( vsac ) during the 1950 college football season. led by fourth - year head coach loyd roberts, the buccaneers compiled an overall a record of 3 – 5 – 1, with marks of 1 – 2 – 1 against smoky mountain opponents and 0 – 1 – 1 in vsac play. this was the program's first losing re cord under roberts and the first losing season since 1941. the team's co - captains were mark sutherland and bob \" snake \" evans. the 1950 squad beat local rival tusculum. they also tied milligan in the final meeting between the\n",
      "\n",
      "[2900, 2001, 2036, 1037, 24646, 2497, 1012, 2017, 2064, 2393, 16948, 2011, 9186, 2009, 1012, 2023, 3720, 2055, 1037, 3075, 1011, 3141, 2311, 2030, 3252, 2003, 1037, 24646, 2497, 1012, 2017, 2064, 2393, 16948, 2011, 9186, 2009, 1012, 1996, 3925, 2264, 2702, 2791, 25212, 2110, 21629, 2374, 2136, 2001, 2019, 2137, 2374, 2136, 2008, 3421, 2264, 5298, 2110, 2267, 1006, 3802, 11020, 1007, 1517, 2085, 2124, 2004, 2264, 5298, 2110, 2118, 1517, 2004, 1037, 2266, 1997, 1996, 20629, 3137, 3034, 1998, 1996, 6951, 2110, 5188, 3034, 1006, 5443, 6305, 1007, 2076, 1996, 3925, 2267, 2374, 2161, 1012, 2419, 2011, 2959, 1011, 2095, 2132, 2873, 8840, 25688, 7031, 1010, 1996, 21629, 9227, 2019, 3452, 1037, 2501, 1997, 1017, 1516, 1019, 1516, 1015, 1010, 2007, 6017, 1997, 1015, 1516, 1016, 1516, 1015, 2114, 20629, 3137, 7892, 1998, 1014, 1516, 1015, 1516, 1015, 1999, 5443, 6305, 2377, 1012, 2023, 2001, 1996, 2565, 1005, 1055, 2034, 3974, 2128, 11601, 2104, 7031, 1998, 1996, 2034, 3974, 2161, 2144, 3874, 1012, 1996, 2136, 1005, 1055, 2522, 1011, 15755, 2020, 2928, 14274, 1998, 3960, 1000, 7488, 1000, 6473, 1012, 1996, 3925, 4686, 3786, 2334, 6538, 10722, 28817, 12942, 1012, 2027, 2036, 5079, 4971, 10762, 1999, 1996, 2345, 3116, 2090, 1996]\n",
      "generated text:\n",
      "japan was also a c hief scout of nepal and patron of the national sports council. in 1987, he was the chairman of the national youth services foundation. in 1974, he represented the king at the coronation of bhutanese king jigme singye wangchuck. prince dhirendra renounced his title of prince and the style of his royal highness because of his relationship with a foreigner in december 1987. after that, he lived in england until he returned to nepal in 1998. on may 13, 1973, he married his second cousin ( and the sister of the wives of his brothers, queen aishwarya and queen komal ), princess prekshya ( 19 january 1956 – 12 nov 2001 ). from this marriage, he had three daughters : in the 1980s, he divorced his first wife. dhirendra was killed in the palace massacre at narayanhiti royal palace, during one of his rare visits to the country. he was due to regain his title and place in the line of succession\n",
      "\n",
      "[2900, 2001, 2036, 1037, 1039, 7632, 12879, 7464, 1997, 8222, 1998, 9161, 1997, 1996, 2120, 2998, 2473, 1012, 1999, 3055, 1010, 2002, 2001, 1996, 3472, 1997, 1996, 2120, 3360, 2578, 3192, 1012, 1999, 3326, 1010, 2002, 3421, 1996, 2332, 2012, 1996, 12773, 1997, 18768, 6810, 2332, 10147, 21693, 2063, 6170, 6672, 7418, 20760, 3600, 1012, 3159, 28144, 7442, 17670, 17738, 22392, 2010, 2516, 1997, 3159, 1998, 1996, 2806, 1997, 2010, 2548, 17114, 2138, 1997, 2010, 3276, 2007, 1037, 29524, 1999, 2285, 3055, 1012, 2044, 2008, 1010, 2002, 2973, 1999, 2563, 2127, 2002, 2513, 2000, 8222, 1999, 2687, 1012, 2006, 2089, 2410, 1010, 3381, 1010, 2002, 2496, 2010, 2117, 5542, 1006, 1998, 1996, 2905, 1997, 1996, 10403, 1997, 2010, 3428, 1010, 3035, 9932, 4095, 9028, 3148, 1998, 3035, 12849, 9067, 1007, 1010, 4615, 3653, 5705, 17915, 1006, 2539, 2254, 3838, 1516, 2260, 13292, 2541, 1007, 1012, 2013, 2023, 3510, 1010, 2002, 2018, 2093, 5727, 1024, 1999, 1996, 3865, 1010, 2002, 9196, 2010, 2034, 2564, 1012, 28144, 7442, 17670, 2001, 2730, 1999, 1996, 4186, 9288, 2012, 24331, 27798, 2548, 4186, 1010, 2076, 2028, 1997, 2010, 4678, 7879, 2000, 1996, 2406, 1012, 2002, 2001, 2349, 2000, 12452, 2010, 2516, 1998, 2173, 1999, 1996, 2240, 1997, 8338]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m text_gen \u001b[39m=\u001b[39m generate_text(model, max_tokens, start_tokens, vocab)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     text_gen\u001b[39m.\u001b[39;49mgenerate()\n",
      "Cell \u001b[0;32mIn[74], line 39\u001b[0m, in \u001b[0;36mgenerate_text.generate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m     x \u001b[39m=\u001b[39m start_tokens\n\u001b[1;32m     38\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([x])\n\u001b[0;32m---> 39\u001b[0m y, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(x, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m sample_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_from(y[\u001b[39m0\u001b[39m][sample_index])\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m sample_token \u001b[39m==\u001b[39m \u001b[39m102\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:61\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_handler\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39mis_traceback_filtering_enabled():\n\u001b[0;32m---> 61\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     63\u001b[0m     filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/keras/engine/training.py:2346\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2344\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2345\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2346\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2348\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1304\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   1305\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1306\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:703\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 703\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:742\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    740\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    741\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 742\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m~/Documents/projects/IRIS/.conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3410\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "sentence = \"Japan was \"\n",
    "start_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "print(start_tokens)\n",
    "max_tokens = 200\n",
    "text_gen = generate_text(model, max_tokens, start_tokens, vocab)\n",
    "for i in range(5):\n",
    "    text_gen.generate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
